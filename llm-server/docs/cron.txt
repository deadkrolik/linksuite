h2. Зачем это нужно

Помимо ваших действий по добавлению сайтов, проектов и ссылок для обеспечения нормальной работы системы необходим регулярный запуск специальных скриптов, автоматизирующих рутинные действия. Ниже перечислены все типы таких заданий и их особенности.

Для их регулярного запуска придумано такое понятие как CRON-задачи. Работа с такими вещами отличается от хостинга к хостингу. Универсальных инструкций дать невозможно, тут нужно спрашивать у суппорта вашего хостера, либо читать его документацию. Общий формат CRONTAB файла таков:
<br>
<pre>
10,20,30,40,50 * * * * wget -O - -q http://сайт.ру/llm-server/cron/имя_задачи
</pre>
Это пример запуска скрипта через каждые десять минут. Если на папке llm-server стоит http-авторизация через *.htpasswd*, то к wget надо добавить дополнительные параметры:
<br>
<pre>
... wget --http-user=имя_пользователя --http-passwd=пароль ...
</pre>

<br>

h2. Тестовый крон

*ИМЯ_ЗАДАЧИ* равно *test.php*.
<br>
Предназначен лишь для проверки правильности запуска крон заданий. Необходимо сделать его первой запускаемой задачей. Минуты запуска  */2 (каждые две минуты, именно так, через дефис и звездочку). Сама же задача всего лишь пишет в log-файл /logs/test.txt начальную строку, усыпает на 10 секунд, пишет еще одну строку и завершается.
<br>

h2. Индексатор сайта

*ИМЯ_ЗАДАЧИ* равно *siteindexer.php*.
<br>
Задача индексатора сайта состоит в том, что бы при своем запуске найти следующий сайт, у которого поднят флажок индексации. После того, как он захватит сайт он создает lock-файл в директории */tmp/*, вида *indexer.SITE_ID.cron* с текущей временной меткой. Периодически он проверяет уже созданные файлы и если процесс почему-то не удалил файл (а это может произойти из-за ограничений по времени выполнения на хостинге), то сайт снимается с индексации и файл удаляется.

Общая логика работы такова:
* Загружаем сайт, готовый к индексации
* Парсим и разбираем подстроки, которые надо исключить из индексации (задается в настройках проекта)
* Получаем файл robots.txt и вытаскиваем все disallow для юзер-агента * (звздочка - это значит все).
* Загружаем конфигурацию из *main.ini*
* Получаем главную страницу и проверяем ее Content-Type, если он не соответсвует разрешенным в файле конфигурации - скрипт завершается
* Собираем адреса на главной странице и сохраняем ее в базу
* Полученные адреса в цикле проверяются на то - внутренние ли они и не запрещена ли их индексация, расширение и тип ответа, если все нормально - адрес считается адресом первого уровня вложенности.
* Для каждой такой страницы собираются все ее исходящие ссылки, а сама страница сохраняется в базу. Собранные урлы считаются урлами второго уровня.
* Тоже самое проделывается и для адресов второго уровня.
* Сохраняется лог индексации сайта.
* Удаляется lock-файл.

Рекомендуемые значения минут для запуска: 10,30,50

h2. Измеритель PR

*ИМЯ_ЗАДАЧИ* равно *prmeter.php*.
<br>
Индексатор построен не на основе общедоступного класса, измеряющего PR для одного адреса. Скрипт умеет подключаться к различным сервисам в интернете, позволяющим расчитывать PR сразу для многих адресов. Плюсов у такого подхода достаточно много - не нужны прокси (если запрашивать PR часто, то google забанит ваш адрес), проверка работает значительно быстрее. Минус один - если переусердствовать, то можно завалить сам сервис или ввести его в бан, что вообще говоря не очень хорошо. Именно поэтому желательно или вообще не запускать проверку PR если она вам не нужна, либо установить как можно больший период запуска. Ведь все равно PR считается один раз, а один раз можно и потерпеть.

Алгоритм работы такой: 
* Загружается конфиг - сколько ждать между циклами и максимально число адресов на цикл
* Загружается пачка адресов, где PR=-1
* Вычисляются полные адреса и эта пачка отдается объекту, умеющему работать с движками
* Результат сохраняется в БД
* Скрипт завершается

Рекомендуемые значения минут для запуска: 15,35,55

h2. Проверка наличия страницы в индексе

*ИМЯ_ЗАДАЧИ* равно *yindex.php*.
<br>
Данная задача занимается рассчетом наличия в индексе страниц сайта. Так как массово проверить это нельзя, то лучше это растянуть на долго, если у вас нет прокси-серверов. При таймауте в 60 секунд за день можно проверить полторы тысячи адресов, поэтому не стоит выставлять меньший период, если вы не хотите быть забанены яндексом за слишком частое общращение к нему.
<br>
Рекомендуемые значения минут для запуска: 0,10,30,40,50

h2. Загрузка файлов со ссылками на хостинги

*ИМЯ_ЗАДАЧИ* равно *ftpupload.php*.
<br>
Данный скрипт предназначен для загрузки файла llm-links.txt, содержащего базу данных ссылок для сайта на хостинги, где запрещены исходящие соединения в PHP.

Алгоритм:
* Загружаем сайт с включенным режимом FTP-загрузки и минимальным временем последнего обращения к нему.
* Ищем URL-адрес выдавателя ссылок и сохраняем файл в папку tmp. Если скрипт вернул ошибку - записываем в лог и выходим.
* Соединяемся с удаленным FTP-хостом и пытаемся загрузить на него файл.
* Удаляем временный файл.

Наилучший период запуска должен быть таков, что бы хотя бы раз в сутки обновить файл на каждом из сайтов. За один проход скрипт берет один сайт. Лог-файлы ошибок хранятся в /tmp/logs/ftpupload.txt.

h2. Проверка проиндексированности ссылок

*ИМЯ_ЗАДАЧИ* равно *checklinkindex.php*.
<br>
Данная задача проверяет наличие уже размещенных ссылок в индексе поисковой системы Яндекс и Google. То есть мало поставить ссылку, надо что бы поисковый робот ее заметил. Результаты труда данного скипта можно увидеть при просмотре ссылок на странице, либо ссылок в задаче. Зеленый кружок с минусом или плюсом как раз и будет означать результат.

Алгоритм:
* Грузим CRON_YLINKINDEX_MAX_LINKS ссылок и проверяем первую на наличие в индексе. Не проверяем ссылки которые проверялись раньше чем сутки назад.
* Получаем результат и записываем в базу время последней проверки и сам результат.
* Усыпаем на CRON_YLINKINDEX_SLEEP секунд и берем следующую ссылку.

Рекомендуемые значения минут для запуска: 0,10,30,40,50

h2. Массовое размещение ссылок

*ИМЯ_ЗАДАЧИ* равно *masslink.php*.
<br>
В форме поиска площадок для проекта есть галочка "Пакетное размещение". Если она выставлена, то ссылки при отправке формы не размещаются, а создается файл крон-задания. Данная задача загружает этот файл и по частям размещает ссылки на отобранных страницах. Число ссылок, размещаемых за раз - это константа CRON_MASSLINK_MAX_LINKS в файле конфигурации.
<br>
Рекомендуемые значения минут для запуска: 0,10,20,30,40,50

h2. Общие рекомендации

После того, как вы добавили в систему все сайты, которые могли, измерили весь PR и проиндексированность, то имеет смысл вообще выключить CRON-задания, дабы не нагружать хостинг бесполезной работой. Берегите природу и электричество!

И еще одно. Возможно я повторюсь, но замечу, что PR вычисляется посредством бесплатных сервисов в интернете. Если слишком переусердствовать с быстрым расчетом PR, то можно вообще пролететь с его измерением, ибо вас забанят или же будут забанены эти сервисы. Хорошо подумайте перед тем как добавлять сайт в систему. Лучше отменить проверку PR (и проиндексированности тоже), если известно, что сайт не имеет никаких показателей.

h2. Устранение ошибок

Если крон завершается с ошибкой "Exited with return code = 127" или вообще не работает, то есть смысл обратиться в суппорт хостинговой вашей компании. Возможно wget запрещен или не установлен на сервере и вам посоветуют другой способ запуска. В любом случае URL запуска остается неизменным, а *wget* может "превратиться" в *fetch*, *get* или *curl*.

h2. Работа через socks

Если вы имеет доступ к socks, то запуск php для того, что бы он ходил в интернет через них возможен при помощи стандартной UNIX-команды:
<br>

bq. */usr/local/bin/socksify /usr/local/bin/php /home/.../llm-server/cron/checklinkindex.php*

<br>
Таким образом можно частично решить часть проблем с баном поисковиками за частые запросы.
